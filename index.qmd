---
title: "Customer Churn Analysis Using Support Vector Machine"
author: "Gopi Shankar Reddy Mallu,
         Kavya Reddy Maale,
         Satya Nageswara Dinesh Donkada,
         Vamsi Krishna Kalla"
date: 'today'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

Support Vector Machines (SVMs) are a type of supervised learning
algorithm that can be used for classification or regression tasks. The
main idea behind SVMs is to find a hyperplane that maximally separates
the different classes in the training data. This is done by finding the
hyperplane that has the largest margin, which is defined as the distance
between the hyperplane and the closest data points from each class. Once
the hyperplane is determined, new data can be classified by determining
on which side of the hyperplane it falls. SVMs are particularly useful
when the data has many features, and/or when there is a clear margin of
separation in the data.

![](images/clipboard-4268047144.png)

Fig: - [Linear & Non-Linear Separable
Data](https://media.geeksforgeeks.org/wp-content/uploads/20200605170732/linearsep.png)

Hyperplanes are decision boundaries that help classify the data points.
Data points falling on either side of the hyperplane can be attributed
to different classes. Also, the dimension of the hyperplane depends upon
the number of features. If the number of input features is 2, then the
hyperplane is just a line. If the number of input features is 3, then
the hyperplane becomes a two-dimensional plane. It becomes difficult to
imagine when the number of features exceeds 3.

![](images/clipboard-4038275398.png)

Fig: - [Hyperplane in 2D & 3D feature
space](https://www.researchgate.net/profile/Mohammed-Kabir/publication/353523194/figure/fig4/AS:1050564217556992@1627485436969/Hyperplanes-in-2D-and-3D-feature-space.png)

## Related Work

Support Vector Machines (SVM) have emerged as a potent tool in the realm
of supervised learning, offering a robust mathematical framework for
both classification and regression tasks. With a foundation rooted in
principles such as structural risk minimization and kernel functions
[(Jakkula,
2006)](https://course.ccs.neu.edu/cs5100f11/resources/jakkula.pdf), SVM
has demonstrated exceptional generalization capabilities, adeptly
handling non-linear decision boundaries through kernel tricks
[(](https://iopscience.iop.org/article/10.1088/1742-6596/1748/5/052006/meta)[Jun,
2021](https://iopscience.iop.org/article/10.1088/1742-6596/1748/5/052006/pdf)[;](https://iopscience.iop.org/article/10.1088/1742-6596/1748/5/052006/meta)
[Deris,
2011)](https://pdf.sciencedirectassets.com/278653/1-s2.0-S1877705811X00206/1-s2.0-S1877705811054993/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDsaCXVzLWVhc3QtMSJHMEUCIQDRIJHxqhNzb10swgpln%2FkY6bjoyZKE3nD%2Fodkt5GI98AIgBTvjYZpQcJgUI1Pqq5f3ukWHMK%2F5XSW98TZuXXQAOcsqsgUIZBAFGgwwNTkwMDM1NDY4NjUiDLPbBsame%2FBmeEwqtSqPBWdTfwj6Vh0LoC5Szxy29pQP1a0DgnSttRO6n%2FSnpCwv4ogGiFM%2B4n0IeQgzc%2B%2BcXJgYdvtBm2Uyl6cACcI2SfdxRJy%2BehNfbVwOmQaFEOlPYHakm1%2FOJb8ZNMSuWZ9cqp2%2FE80YsnL30bU%2Fz8uFQpQ4YQlUgFpHdNV8JivmOQo2CfvVblKzQsEcDcecoOZhkGlLx1ES3veMI2hjHpbLYgsUU1wWV39N3fjGVvUMsLq9N7MwYumyhhMkCnTWWLBdl2Lu3EhvSyfkvJwNcac8ldPC2WenhwWcq3SnfqRQE3lNjDZSB3yrVQVDlSA5ec2g0qx7jGrwZu8vagM%2Ffoag0WRnRQWU%2BzFDJnDxhES21QUOcOyvMpsC3BLhJ60kWjxWJYhGv5l8frXArc6rI%2B%2BRxj7OKM7AEW4I3t%2Bju%2BzR7LWpwHNe7XdpQ32i3V6cZFU7ghm58Pw8Tj2aK8ahkn6Jra%2BWTIUZCJHyh9yKDNolxX%2FqfOtCC%2Bmz%2F%2Bf0tkI%2BLR2BuI1sazmeTUG5MkCb8poIMu6f81XRVBcHzdBKCFQbXnAEx5WQlLTiwo%2FtmNVL5JQbPfz7cFjyHr%2BpLWvZkraWM9Kd0D0MD1oIIUem5T9QnrFfpjBHhYRS6v%2F%2B%2FE5C6%2BdxE%2B4UJl0B3P%2FG2pJiyj4I2yQ%2FnFBaNE%2BmzhAS9K4Dr7lcANySJx0cwm%2Fxeo4pgXY9iuxOpPlSJvncOrmVL0hORsR4JztZud%2BJGtfnDMJpAkoLieh89XrfzR3HbM5Vsn4z6r0OJoqauVdmtshtt7V2f3oDsyrCUTnqQb2rwFHcEEYtDNLIEiTdh9j%2F12pGPD9KxJ6X1lIxEBmn305qUNzh87k3DNS3Oxvl2Md%2BC7FC2PMw8amxsAY6sQG1s1mPdL8wLhYf4hi2NdVDc6si0RhjSLc94rpZRLlOJIqLAiva40Jd6Pzsus2Itc5I25yb%2FRqHZ9MWyBoasFU0q0GWZkgLw5Xf3yknNCjO7DlJtUVCAeO1Iw0bP9xp0dvstHZ7nwdhWSBXSx%2B9rozQn%2BaobFRLa0lYQOJ%2Bs7tnUqf8XlhkpIGMX9hNUC86LSIiEx30a45wzXMyMiOC7%2B9cp3D1dRzq9wPZc3QBCiq7K3Q%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240402T195007Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYVCPHGYRP%2F20240402%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=d428a97615b79c27a1060daf99291736e85ff1c9ce89cbffecb00cf8c4a00110&hash=a71efd70741dddce15851eeb002b23c90a18f2723f367cb1df13177c2952071f&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1877705811054993&tid=spdf-ba9c0247-90b6-4b16-9817-1575489d723b&sid=cdec0a5829d248405139cb239ff75e85058egxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1315585704525a565d5d&rr=86e358999b15244b&cc=us).
Despite challenges like computational cost and scalability [(Bhavsar,
2012)](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=d683a971524a0d76382ce335321b4b8189bc8299),
the evolution of SVM has led to significant contributions in diverse
fields, including pattern recognition, computer vision [(Kecman,
2005)](https://link.springer.com/chapter/10.1007/10984697_1), and even
agriculture, where it aids in optimizing crop yield and disease
identification [(Kumar et al.,
2017)](https://pdf.sciencedirectassets.com/280203/1-s2.0-S1877050917X00215/1-s2.0-S1877050917326984/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDsaCXVzLWVhc3QtMSJHMEUCIQDrbzxUNjQ6xkuKOG3uSbmno3adY4Ajq6GGLvDskWQY8gIgL3Q%2Fb3xMXBtanZ4%2BVrSiA2dhG1S3kLsC97YTqR7XYl0qswUIZBAFGgwwNTkwMDM1NDY4NjUiDMKgGC1WeK5hj7UX6yqQBWj%2F7RYNo1o46RxiemZDXwc9nWsiwFmRpSzTOl8IHlrApNULw8DnanHeg1xvw3EB6kSckNuYksIJnEUs9bLfhQKEp9roNxbg%2FPiXo3OgXcqmhMfFuR1lNDcgPLw1RSitBgym72wwWPCqkH9ktw57r3TwV5%2BC%2BV6k1TeoGA4FU4gSqIlnDcKpe3uUy61sLylmpLJGgwMLt1ycp0Y1c66loqSdmnEDlKex7K%2BEyEYLr2U%2BiQmB5M%2F6hluTs5IOkmLYTZI%2F22J3Ctq%2Fpx18RCblT4%2B%2FR9dmmJNG3MuybIOgT2YOrqGKHvi4ODxOG11rVRQcjSg952Cud%2BNc2bBJo1zWqFXKfgBcLFQMeEltPLhXLa65%2BfUAabOx7W3pcKqrWzpuZzl1BIjbbihGUFhNsnqSMz%2B%2FbyOEMnNk6Vmsg7x5cVudz1%2Bj3VYsN6BeSeYOtwYKYwMkMZwASfrmsjYMcUwDO0291OBvJzCeckH4jtUV4YgLvNya9MLbJlpFX997yH%2FrE7EswVfmaTLb2VDQc6OdkvjHQHZ4hthqWYxaO1jQNG5l84vKwgsONMLlwAEuyuxKYJJnLzILg1dt%2BM6BUY42SD%2FdQGiCVTkOAEPloeT80EJu4wKTDi8GcZtjr%2F63ZWRFOYtN9A1NKUwHob2SrSneoM0R04%2BuUKyhzh2w%2FlJAO3T0onHNDoZvqYHND2udT38smpGqKAx4gorf7dBPT8WUVnsUhpNsensw4IhXKgyy2LeMQPpogx%2FZKVaRKvi%2FZNrXkGWEq8vch3TKkL2uJOrczRuMu6bIv3DQke6XKKyUdjkfVJ2V5GCieBj80TSJ6ijqUWn0JlkRj%2BwS9Ybnq53zY%2FmGCdsv633uHBkePfoUUxJvMIKasbAGOrEBBxOxxGNHJ9oRRL9H492fVHuzi8TlF50gY8xFRqujQf%2F6Mrh%2FNM5vtsBF6i56bP6UmGme9rpBBAfreXLBsU1QTqisXacErYZ26xAFp3Y5jAxYgaobjIP9WlWXQBQKrngf%2Bp7V4H8L6ugX%2F4ucgPb%2BOp8UaZdl17Q5wcKpQampwZmc7BSoKdnO9s787yngNjLYTa7Y6WGRJL%2BZ2pgDJdR%2FqFan%2BCdO8uIe%2BSHVNQ6q45X0&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240402T195145Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYRMTTYZMG%2F20240402%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=13ee174eeb82a425e46de119799160f19e618179b2722928d09f536851377cc3&hash=a19640cc6168c6ce9d2535df2fe6efc7c052d37fcbbc4ed0382222820c77475d&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1877050917326984&tid=spdf-0546f9e9-4383-4cad-a6f2-d1461c2d5949&sid=cdec0a5829d248405139cb239ff75e85058egxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1315585704525a0f025d&rr=86e35af9d851244b&cc=us).
The ongoing advancements in SVM research are geared towards refining
algorithms and broadening their application spectrum, especially in the
context of burgeoning data volumes [(Yue,
2003)](https://link.springer.com/article/10.1007/s11766-003-0059-5).

In the financial and healthcare sectors, SVM has proven its efficacy in
various applications. It has been utilized to construct reliable stock
market prediction models by analyzing financial indices like Earnings
Per Share (EPS) and Net Profit Growth Rate (NPGR) [(Han,
2007)](https://scholarworks.lib.csusb.edu/cgi/viewcontent.cgi?article=1059&context=ciima).
In healthcare, SVM has been instrumental in developing advanced
diagnostic tools, such as the optimized SVM model for early dementia
prediction [(Javeed et al.,
2023)](https://www.mdpi.com/2227-9059/11/2/439) and the multi-disease
prediction model using an improved SVM-radial bias kernel approach
[(Harimoorthy & Thangavelu,
2021)](https://www.researchgate.net/publication/338353198_Multi-disease_prediction_model_using_improved_SVM-radial_bias_technique_in_healthcare_monitoring_system#fullTextFileContent).
These innovations underscore the potential of machine learning in
revolutionizing healthcare by facilitating early diagnosis and
personalized treatment plans.

SVM's application extends to domains like online retail and network
security, where it addresses complex challenges with remarkable
efficiency. In online marketplaces, SVM combined with Particle Swarm
Optimization has enhanced the accuracy of text classification for
customer reviews [(Sahara et al.,
2023)](https://doi.org/10.1063/5.0129404), providing valuable insights
for sellers. In the realm of network security, innovative approaches
such as combining SVM with na√Øve Bayes feature embedding have been
proposed for intrusion detection, achieving high accuracy rates in
identifying network threats [(Jie Gu et al,
2021)](https://www.sciencedirect.com/science/article/abs/pii/S0167404820304314?via%3Dihub).
Moreover, the development of hybrid methods for attack detection, which
integrate SVM features with evolutionary algorithms and artificial
neural networks, has shown significant promise in reducing
dimensionality and training time while maintaining high detection
accuracy [(Soodeh Hosseini et al,
2020)](https://www.sciencedirect.com/science/article/abs/pii/S1389128619302191?via%3Dihub)

Machine learning techniques, particularly SVM, are revolutionizing
various fields by addressing complex challenges with precision and
efficiency. In healthcare, SVM has been applied to electronic health
records for cancer classification, achieving high accuracy rates in
identifying different types of malignancies [(K. Ghanem et al,
2021)](https://ieeexplore.ieee.org/abstract/document/8233268).
Furthermore, SVM's versatility is evident in its application across
domains such as finance, where it has been used to assess credit risk
for small and medium enterprises in supply chain finance [(Zhang, Hu, &
Zhang, 2015)](https://doi.org/10.1186/s40854-015-0014-5), and in
cloud-based services, where it ensures data confidentiality and decision
verifiability in health monitoring systems [(Liang et al.,
2021)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9415746).
These advancements highlight the transformative potential of machine
learning techniques in enhancing diagnostic accuracy, optimizing
financial assessments, and ensuring secure cloud-based services.

## Methods

**Mathematical Intuition of Support Vector Machine**

Consider a binary classification task where there are two classes,
denoted by the labels +1 and -1. The input feature vectors (X) and the
matching class labels (Y) comprise our training dataset.

Equation for hyperplane can be written as:

$$
w^Tx+b=0
$$ {#eq-hyperplane}

The vector W represents the normal vector to the hyperplane. i.e the
direction perpendicular to the hyperplane. The parameter **b** in the
equation represents the offset or distance of the hyperplane from the
origin along the normal vector **w**.

$$
d_i = \frac{w^Tx_i + b}{\|w\|}
$$ {#eq- distance of hyperplane}

where \|\|w\|\| represents the Euclidean norm of the weight vector w.
Euclidean norm of the normal vector W

$$
\hat{y} =\begin{cases} 0 & \text{if } w^T x + b \geq 0 \\ 1 & \text{if } w^T x + b < 0 \end{cases}
$$ {#eq-Euclidean norm}

**kernel function in SVM**

In Support Vector Machines (SVM), the kernel function plays a crucial
role in transforming the input feature space into a higher-dimensional
space where the data can be linearly separated. This is particularly
useful in cases where the data is not linearly separable in its original
space. The kernel function computes the dot product between the feature
vectors in this higher-dimensional space without explicitly mapping the
vectors into that space, which is known as the "kernel trick."

Common types of kernel functions include:

-   **Linear Kernel**: $K(w,b)=w^Tx+b$. This is the simplest form of the
    kernel, used when the data is linearly separable.

-   **Polynomial Kernel**: $K(w, b) = (1 + w^T.x b)^d$. This kernel maps
    the input features into a polynomial feature space, allowing for
    polynomial decision boundaries.

-   **Radial Basis Function (RBF) Kernel**:
    $K(w, b) = \exp(-\gamma |w.x - b|^2)$. Also known as the Gaussian
    kernel, it maps the features into an infinite-dimensional space,
    providing a lot of flexibility for non-linear decision boundaries.

Each kernel function has its own set of parameters that need to be tuned
for optimal performance. The choice of kernel function and its
parameters can significantly impact the SVM model's ability to capture
the underlying patterns in the data.

#### **Margin and Support Vectors**

The margin in SVM is defined as the distance between the separating
hyperplane and the nearest data points from each class, known as the
support vectors. The goal of SVM is to find the hyperplane that
maximizes this margin, as a larger margin is associated with better
generalization ability of the model.

Support vectors are the data points that lie closest to the decision
boundary and are critical in defining the position and orientation of
the hyperplane. These are the points that directly influence the shape
of the decision boundary, as any small change in their position can
alter the hyperplane. The SVM model is said to be "sparse" because only
the support vectors contribute to defining the hyperplane, while other
data points have no influence.

#### **Objective Function and Optimization**

The objective function that SVM optimizes is a combination of maximizing
the margin and minimizing the classification error. This is achieved
through the minimization of the following objective function:

$$
min_{w, b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{n} \xi_i
$$

Subject to the constraints:

$$
y_i (w^T x_i + b) \geq 1 - \xi_i \quad \text{and} \quad \xi_i \geq 0 \quad \text{for all } i
$$

where $w$ is the weight vector, $b$ is the bias term, $C$ is the
regularization parameter, $\xi_i$ are the slack variables representing
the degree of misclassification of the $i$-th data point, and $y_i$ are
the class labels.

The hinge loss function is used in SVM to penalize misclassifications.
It is defined as:

**Hinge loss** = $\max(0, 1 - y_i (w^T x_i + b))$

The hinge loss is zero for correctly classified points that are outside
the margin, and it increases linearly for points that are on the wrong
side of the hyperplane or within the margin.

The optimization of the objective function involves finding the values
of $w$ and $b$ that minimize the function, subject to the constraints.
This is typically done using quadratic programming techniques.

## Dataset

Customer retention is a critical aspect for banks to ensure the
sustainability of their operations. ABC Multinational Bank, in
particular, places a strong emphasis on retaining its account holders.
The primary objective of this analysis is to examine the customer data
of the bank's account holders to predict and prevent customer churn
effectively.

The dataset under consideration contains information about account
holders at ABC Multinational Bank, with the ultimate goal of predicting
customer churn. The dataset comprises the following columns:

| Column Name        | Description                                                                        |
|-----------------|-------------------------------------------------------|
| `customer_id`      | A unique identifier for each customer, not used in the analysis.                   |
| `credit_score`     | A numerical representation of the customer's creditworthiness.                     |
| `country`          | The country in which the customer resides.                                         |
| `gender`           | The gender of the customer (e.g., male, female).                                   |
| `age`              | The age of the customer in years.                                                  |
| `tenure`           | The number of years the customer has been with the bank.                           |
| `balance`          | The current balance in the customer's account.                                     |
| `products_number`  | The number of products the customer has with the bank.                             |
| `credit_card`      | Indicates whether the customer has a credit card with the bank.                    |
| `active_member`    | Indicates whether the customer is an active member.                                |
| `estimated_salary` | The estimated annual salary of the customer.                                       |
| `churn`            | The target variable, indicating customer churn (1 for churned, 0 for not churned). |

Source: - [Bank Churn
Dataset](https://www.kaggle.com/datasets/rangalamahesh/bank-churn/data)

## **Analysis and Results**

**Exploratory Data Analysis (EDA)**

Our Exploratory Data Analysis (EDA) aimed to uncover patterns, detect
anomalies, and test hypotheses about our data. We started with summary
statistics to understand the central tendency, dispersion, and shape of
the dataset's distributions. For instance, we observed that the Credit
Score ranged from 350 to 850, with a median of 659, and the Age of
customers varied from 18 to 92 years, with a median age of 37 years.

We then proceeded to visualize the distribution of key variables using
distribution plots. This helped us identify the skewness in the 'Age'
distribution and the uniform distribution of 'Estimated Salary.' Pair
plots were employed to explore the relationships between variables like
'Age' vs. 'Estimated Salary' and 'Age' vs. 'Credit Score,' providing
insights into how different factors might influence customer churn.

Through our EDA, we also investigated the distribution of the target
variable 'churn' across different geographical regions and examined how
the number of products varied across different regions. Correlation
plots were utilized to identify potential relationships between
features, revealing a positive correlation between 'Age' and 'Balance,'
and a negative correlation between 'NumOfProducts' and 'Balance.'

**Feature Engineering**

In our feature engineering process, we transformed the 'Gender' column
from categorical to numerical by encoding 'Male' as 1 and 'Female' as 0.
We also applied one-hot encoding to the 'Geography' column to convert it
into binary variables for each country, ensuring that our model could
interpret these categorical features correctly. Additionally, we split
our data into training and testing sets to evaluate the performance of
our models on unseen data. To address class imbalance in our target
variable, we employed the Synthetic Minority Over-sampling Technique
(SMOTE), which helped create a more balanced distribution of classes.
Finally, we scaled our data using the StandardScaler to ensure that all
features contributed equally to the model's performance, preventing any
feature with larger values from dominating the model's learning process.

**Loading Libraries**

```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
#install.packages("corrplot")
library(corrplot)
library(caret)
library(smotefamily)
library(ROSE)
library(caret)
library(DMwR)
library(e1071)
library(pROC)
library(doParallel)
library(foreach)
library(randomForest)  # For Random Forest
library(xgboost)
```

**Load Data**

```{r}
df <- read.csv("dataset/train.csv")
```

```{r}
dim(df)
```

**Summary Statistics**

```{r}
summary(select(df, CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary))
```

**Credit Score:**

-   The Credit Score ranges from a minimum of 350 to a maximum of 850.

-   The median Credit Score is 659, indicating that half of the
    customers have a score below 659 and half have a score above.

-   The mean Credit Score is approximately 656.5, suggesting that the
    average creditworthiness of customers is in the mid-range.

-   The 1st quartile (25th percentile) is 597, and the 3rd quartile
    (75th percentile) is 710, indicating that 50% of customers have a
    Credit Score between 597 and 710.

**Age:**

-   The Age of customers ranges from 18 to 92 years. The median age is
    37 years, meaning half of the customers are younger than 37 and half
    are older.

-   The mean age is approximately 38.13 years, indicating that the
    average customer is in their late thirties.

-   The distribution of Age is slightly right-skewed, as the mean is
    slightly higher than the median.

**Tenure:**

-   Tenure, or the number of years customers have been with the bank,
    ranges from 0 to 10 years.

-   The median tenure is 5 years, indicating that half of the customers
    have been with the bank for less than 5 years and half for more.

-   The mean tenure is approximately 5.02 years, suggesting that the
    average customer has been with the bank for around 5 years.

**Balance:**

-   The account Balance ranges from a minimum of 0 to a maximum of
    250,898.

-   The median balance is 0, indicating that at least half of the
    customers have no balance in their account.

-   The mean balance is approximately 55,478, suggesting that while many
    customers have low or zero balances, some have significant amounts
    in their accounts.

**Number of Products:**

-   The Number of Products customers have with the bank ranges from 1 to

    4.  

-   The median number of products is 2, meaning that half of the
    customers have 2 or fewer products with the bank.

-   The mean number of products is approximately 1.554, indicating that
    on average, customers have between 1 and 2 products with the bank.

**Estimated Salary:**

-   The Estimated Salary ranges from a minimum of 11.58 to a maximum of
    199,992.48.

-   The median estimated salary is 117,948, suggesting that half of the
    customers have an estimated salary below this amount and half above.

-   The mean estimated salary is approximately 112,574.82, indicating
    that the average estimated salary of customers is around 112k.

**Count Of Categorical value types**

```{r}
sapply(df[,c('Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'Exited')], function(x) length(unique(x)))
```

**Checking null values**

```{r}
colSums(is.na(df))
```

There are no null values in the data.

**Distribution of target variable**

```{r}
table(df$Exited)
```

We can see number of customers exited are more compared to number of
customers not exited. So there is a quite imbalance in data which needs
to be addressed while building the model.

**Distribution of target variable across Geography.**

```{r}

table(df$Geography, df$Exited)

```

**France:**

-   A total of 94,215 customers are from France.
-   Out of these, 78,643 customers have not exited the bank (retained),
-   while 15,572 customers have exited (churned).
-   The churn rate for France is approximately 16.53%.

**Germany:**

-   A total of 34,606 customers are from Germany.
-   Out of these, 21,492 customers have not exited the bank, while
    13,114 customers have exited.
-   The churn rate for Germany is approximately 37.89%.

**Spain:**

-   A total of 36,213 customers are from Spain.
-   Out of these, 29,978 customers have not exited the bank, while 6,235
    customers have exited.
-   The churn rate for Spain is approximately 17.21%.

**Which Gender has highest Credit Score?**

```{r}
aggregate(df$CreditScore, by = list(df$Gender), FUN = mean)
```

**Observations:**

-   The difference in average credit scores between male and female
    customers is minimal, indicating that gender does not significantly
    impact creditworthiness in this dataset.

-   Both genders have an average credit score in the mid-650s, which is
    considered a fair credit score range.

**Distribution of Age.**

```{r}
ggplot(df, aes(x = Age)) + geom_histogram(binwidth = 5, fill = "blue", color = "black")
```

**Observations:**

-   The largest concentration of customers falls within the 30 to
    40-year-old range, indicating that the majority of customers are in
    their early to mid-career stages.

-   There is a significant drop in frequency as age increases,
    especially beyond 50 years. This suggests that the customer base
    skews younger.

-   The distribution is right-skewed, meaning there are fewer older
    customers (those over 60) compared to younger customers.

-   There is a small number of customers in the youngest age bracket
    (under 25 years) and the oldest (over 75 years).

**Distribution of Estimated Salary:**

```{r}
ggplot(df, aes(x = EstimatedSalary)) + geom_histogram(binwidth = 5, fill = "blue", color = "black")
```

**Observations:**

-   The distribution is quite uniform across different salary ranges,
    with no distinct peaks that would indicate a concentration of
    individuals around a specific salary bracket.

-   There are frequent spikes throughout the distribution, which may
    suggest that the data contains many unique values with small
    frequencies. This could be indicative of precise salary estimations
    rather than rounded figures.

-   The salaries range from very low values close to 0 up to 200,000,
    indicating a diverse group from potentially different economic
    backgrounds or job roles.

-   There is no obvious concentration of data points around the lower,
    middle, or upper salary range, which is unusual for income data
    where one typically expects to see more of a bell-shaped
    distribution centered around a median salary range.

**Comparing the distribution of account balances between customers who
have exited and customer who have not exited.**

```{r}
ggplot(df, aes(x = as.factor(Exited), y = Balance)) + geom_boxplot()
```

**Observations:**

-   **Balance Distribution:**

    -   The y-axis represents the balance on customer accounts, which
        seems to range from 0 to a bit over 250,000.

    -   Both boxes have a similar interquartile range (IQR), which is
        the range between the first quartile (25th percentile) and the
        third quartile (75th percentile), represented by the height of
        the boxes. This suggests that the middle 50% of balances are
        similarly distributed between both groups.

    -   The median, indicated by the line within each box, is roughly at
        the same level for both groups, suggesting that the central
        tendency of balance is similar regardless of whether the
        customer has exited or not.

-   **Outliers:**

    -   There are visible outliers for both groups, indicated by the
        points beyond the whiskers of the box plot. These outliers
        represent customers with balances significantly higher than the
        general population of the dataset.

**How the distribution of the number of products varies across different
geographical regions?**

```{r}
ggplot(df, aes(x = Geography, fill = as.factor(NumOfProducts))) + geom_bar(position = "dodge")
```

**Observations:**

1.  **France:**

    -   France has the highest count of customers using one product,
        followed closely by those using two products. The number of
        customers using three and four products is significantly lower.

2.  **Germany:**

    -   Germany shows a similar pattern to France with one and two
        products being the most common among customers. However, the
        count for one product is notably lower than in France, whereas
        the count for two products is slightly higher.

3.  **Spain:**

    -   Spain's pattern mirrors that of France and Germany, with one
        product being the most common, followed by two products. Again,
        three and four products are used by a considerably smaller
        number of customers.

**Pairplot of Age vs Estimated Salary and also checking which age group
and salary range have exited the bank.**

```{r}
ggplot(df, aes(x = Age, y = EstimatedSalary, color = as.factor(Exited))) + geom_point()
```

**Observations:**

1.  There doesn't appear to be a clear pattern or correlation between
    Age and Estimated Salary with customer churn, as the exited and
    non-exited customers are interspersed throughout the plot without
    any distinct clustering.

2.  Customers who have exited are spread across all ages and salary
    levels, but there seems to be a slightly higher concentration of
    churned customers in the 40 to 50 age range.

**Pairplot of Age vs Credit Score and also checking which age group and
Credit Score range have exited the bank.**

```{r}
ggplot(df, aes(x = Age, y = CreditScore , color = as.factor(Exited))) + geom_point()
```

**Observations:**

1.  There is a wide distribution of Credit Scores across different ages
    with no clear pattern indicating that Credit Score by itself may not
    be a strong predictor of customer exit.

2.  Both exited and non-exited customers are found across the entire
    range of Credit Scores and Age, but there is a noticeable density of
    exited customers (blue dots) in the middle age range, particularly
    between ages 40 and 50.

**Pairplot of EstimatedSalary vs Credit Score and also checking what
Estimated Salary range and Credit Score range have exited the bank.**

```{r}
ggplot(df, aes(x = EstimatedSalary, y = CreditScore , color = as.factor(Exited))) + geom_point()
```

**Observations:**

The scatter plot shows no clear correlation between Credit Score and
Estimated Salary in predicting customer churn, with both customers who
exited and those who did not evenly dispersed across all ranges of
Salary and Credit Scores.

#### **Correlation Plot**

```{r}
corr_matrix <- cor(select(df, CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary))
corrplot(corr_matrix, method = "circle")
```

**Observations:**

There seems to be a noticeable positive correlation between Age and
Balance, and a negative correlation between NumOfProducts and Balance.

#### **Churn Rate by Geography**

```{r}
churn_by_country <- df %>%
  group_by(Geography) %>%
  summarise(
    Total_Customers = n(),
    Churned_Customers = sum(Exited),
    Churn_Rate = (sum(Exited) / n()) * 100
  )
```

```{r}
ggplot(churn_by_country, aes(x = Geography, y = Churn_Rate, fill = Geography)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Churn_Rate, 2)), vjust = -0.3) +
  labs(title = "Churn Rate by Country",
       x = "Geography",
       y = "Churn Rate (%)") +
  theme_minimal() +
theme(legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5)) 
```

```{r}
sample_size <- 20000
sample <- createDataPartition(df$Exited, p = sample_size / nrow(df), list = FALSE)
df <- df[sample, ]
```

```{r}
df <- df[, !(names(df) %in% c('id', 'CustomerId', 'Surname'))]
df$Gender <- ifelse(df$Gender == 'Male', 1, 0)
```

```{r}
df <- cbind(df, model.matrix(~ Geography - 1, data = df))
df <- df[, !names(df) %in% "Geography"]
```

#### Split data in to train and test

```{r}
set.seed(123)  # For reproducibility
splitIndex <- createDataPartition(df$Exited, p = 0.8, list = FALSE)

train <- df[splitIndex, ]
test <- df[-splitIndex, ]
test_data <- test[, -which(names(test) == "Exited")]
```

```{r}
names(train)
```

```{r}
dim(train)
```

```{r}
table(train$Exited)
```

#### Handle Class Imbalance

```{r}
train_balanced <- ovun.sample(Exited ~ ., data = train, method = "both", N = 16000, p = 0.5)$data
```

```{r}
table(train_balanced$Exited)
```

#### Scaling Data

```{r}
# Select numerical columns for normalization
numerical_cols <- c("CreditScore", "Age", "Tenure", "Balance", "NumOfProducts", "EstimatedSalary")

# Compute the mean and standard deviation from the training set
means <- apply(train_balanced[numerical_cols], 2, mean, na.rm = TRUE)
sds <- apply(train_balanced[numerical_cols], 2, sd, na.rm = TRUE)

# Normalize the training set
train_balanced[numerical_cols] <- sweep(train_balanced[numerical_cols], 2, means, "-")
train_balanced[numerical_cols] <- sweep(train_balanced[numerical_cols], 2, sds, "/")

# Normalize the test set using the same parameters
test[numerical_cols] <- sweep(test[numerical_cols], 2, means, "-")
test[numerical_cols] <- sweep(test[numerical_cols], 2, sds, "/")
```

## Statistical Modelling

In this modeling phase, three different machine learning models are
trained for a classification task using the `caret` package in R. The
models include a Support Vector Machine (SVM) with a radial basis
function kernel, a Random Forest model, and an XGBoost model.

The target variable `Exited` is converted to a factor to ensure that it
is treated as a categorical variable for classification. A consistent
seed (`set.seed(123)`) is set before training each model to ensure
reproducibility of the results.

The `trainControl` function is used to set up the training control
parameters, specifying 10-fold cross-validation (`method = "cv"`) to
assess the performance of the models.

For the SVM model, data preprocessing steps
(`preProcess = c("center", "scale")`) are included to center and scale
the features, which is often necessary for SVM models to perform well.

Each model is trained on the `train_balanced` dataset using the `train`
function from the `caret` package, with the model type specified by the
`method` parameter (`"svmRadial"` for SVM, `"rf"` for Random Forest, and
`"xgbTree"` for XGBoost).

The trained models are stored in a list named `models` for easy access
and further evaluation. This modular approach allows for straightforward
comparison of the performance of different models on the same dataset.

```{r}
train_control <- trainControl(method = "cv", number = 5)

# Set up a list to store models
models <- list()

# Train an SVM model
train_balanced$Exited <- as.factor(train_balanced$Exited)
set.seed(123)
models$svm <- train(Exited ~ ., 
                    data = train_balanced, 
                    method = "svmRadial",  # Radial basis function kernel
                    trControl = train_control,
                    preProcess = c("center", "scale"))

# Train a Random Forest model
set.seed(123)
models$random_forest <- train(Exited ~ ., 
                               data = train_balanced, 
                               method = "rf",  # Random Forest
                               trControl = train_control)

# Train an XGBoost model
set.seed(123)
models$xgboost <- suppressWarnings(train(Exited ~ ., 
                        data = train_balanced, 
                        method = "xgbTree",  # XGBoost
                        trControl = train_control))

```

```{r}

for (model_name in names(models)) {
    set.seed(123)
  
    cat("****** Model:", model_name, " ******\n")
    # Make predictions
    test_data <- test[, -which(names(test) == "Exited")]
    predictions <- predict(models[[model_name]], test_data)
    predictions_factor <- factor(predictions, levels = c("0", "1"))
    exited_factor <- factor(test$Exited, levels = c("0", "1"))
    probabilities <- predict(models[[model_name]], test_data, type = "prob")[,2]
    confusion_matrix <- confusionMatrix(predictions_factor, exited_factor)
    accuracy <- confusion_matrix$overall['Accuracy']
    f1_score <- confusion_matrix$byClass['F1']
    
    # Calculate AUC-ROC
    roc_curve <- roc(response = test$Exited, predictor = as.numeric(predictions))
    auc_roc <- auc(roc_curve)
    
    # Print metrics
    print(confusion_matrix)
    cat("AUC-ROC:", auc_roc, "\n")
    
    # Plot AUC-ROC curve
    plot(roc_curve, main = paste("AUC-ROC Curve for", model_name))
    
    cat("\n")
}
```

## Conclusion

In this analysis of bank customer churn prediction, the Support Vector
Machine (SVM) model has shown promising results, particularly in terms
of specificity (81.09%) and positive predictive value (94.02%). These
metrics are crucial in the banking context, as they indicate the model's
accuracy in correctly identifying loyal customers (specificity) and its
reliability in flagging potential churners (positive predictive value).
Additionally, the SVM model's highest AUC-ROC score of 0.801 underscores
its effectiveness in distinguishing between customers who are likely to
churn and those who are not across various decision thresholds.

Although the Random Forest model exhibited the highest overall accuracy
(83.38%) and kappa score (0.5375), its lower specificity and negative
predictive value compared to the SVM model suggest it may produce more
false positives, leading to misallocated retention efforts. The XGBoost
model showed balanced performance with a slight edge in sensitivity
(81.96%) but did not surpass the SVM model in AUC-ROC.

| Metric                    | SVM    | Random Forest | XGBoost |
|---------------------------|--------|---------------|---------|
| Accuracy                  | 79.52% | 83.38%        | 80.78%  |
| Kappa                     | 0.494  | 0.5375        | 0.5017  |
| Sensitivity               | 79.11% | 86.48%        | 81.96%  |
| Specificity               | 81.09% | 71.70%        | 76.34%  |
| Positive Predictive Value | 94.02% | 91.99%        | 92.86%  |
| Negative Predictive Value | 50.82% | 58.54%        | 52.97%  |
| Balanced Accuracy         | 80.10% | 79.09%        | 79.15%  |
| AUC-ROC                   | 0.801  | 0.791         | 0.791   |

```         
```

## References

1.  Jakkula, V. (2006). Tutorial on support vector machine
    (svm).¬†*School of EECS, Washington State University*,¬†*37*(2.5), 3.

2.  Kecman, V. (2005). Support vector machines-an introduction. In
    Support vector machines theory and applications (pp. 1-47). Berlin,
    Heidelberg: Springer Berlin¬†Heidelberg

3.  Yue, S., Li, P., & Hao, P. (2003). SVM classification: Its contents
    and challenges. Applied Mathematics-A Journal of Chinese
    Universities, 18, 332-342.

4.  Jun, Z. (2021). The development and application of support vector
    machine. In¬†*Journal of Physics: Conference Series*¬†(Vol. 1748, No.
    5, p. 052006). IOP Publishing.

5.  Bhavsar, H., & Panchal, M. H. (2012). A review on support vector
    machine for data classification. International Journal of Advanced
    Research in Computer Engineering & Technology (IJARCET), 1(10),
    185-189.

6.  Deris, A. M., Zain, A. M., &¬†Sallehuddin, R. (2011). Overview of
    support vector machine in modeling machining performances.¬†*Procedia
    Engineering*,¬†*24*, 308-312.

7.  Han, Shuo. ‚ÄúUsing SVM with Financial Statement Analysis for
    Prediction of Stocks.‚Äù *Communications of the IIMA Communications of
    the IIMA*, vol. 7, 2007,
    scholarworks.lib.csusb.edu/cgi/viewcontent.cgi?article=1059&context=ciima.

8.  Ahmadi, Muhammad Iqbal, et al. ‚ÄúSENTIMENT ANALYSIS ONLINE SHOP on
    the PLAY STORE USING METHOD SUPPORT VECTOR MACHINE (SVM).‚Äù *Seminar
    Nasional Informatika (SEMNASIF)*, vol. 1, no. 1, 15 Dec. 2020, pp.
    196‚Äì203, jurnal.upnyk.ac.id/index.php/semnasif/article/view/4101.
    Accessed 13 Feb. 2024.

9.  Razzaghi, Talayeh, et al. ‚ÄúMultilevel Weighted Support Vector
    Machine for Classification on Healthcare Data with Missing Values.‚Äù
    *PLOS ONE*, vol. 11, no. 5, 19 May 2016, p. e0155119,
    https://doi.org/10.1371/journal.pone.0155119.

10. √ñz, Ersoy, and H√ºseyin Kaya. ‚ÄúSupport Vector Machines for Quality
    Control of DNA Sequencing.‚Äù *Journal of Inequalities and
    Applications*, vol. 2013, no. 1, 4 Mar. 2013,
    https://doi.org/10.1186/1029-242x-2013-85. Accessed 15 June 2021.

11. ‚ÄúSupport Vector Machine for Network Intrusion and Cyber-Attack
    Detection \| IEEE Conference Publication \| IEEE Xplore.‚Äù
    *Ieeexplore.ieee.org*,
    ieeexplore.ieee.org/abstract/document/8233268. Accessed 13 Feb 2024.

12. Kumar, Sachin, et al. ‚ÄúPrecision Sugarcane Monitoring Using SVM
    Classifier.‚Äù *Procedia Computer Science*, vol. 122, 2017, pp.
    881‚Äì887, https://doi.org/10.1016/j.procs.2017.11.450. Accessed 25
    July 2019.

13. Javeed, A. et al. (2023) Early prediction of dementia using feature
    Extraction Battery (FEB) and optimized support vector machine (SVM)
    for Classification, MDPI. Available at:
    https://www.mdpi.com/2227-9059/11/2/439 (Accessed: 22 January 2024).

14. Nawal, Y., Oussalah, M., Fergani, B., & Fleury, A. (2022). New
    incremental SVM algorithms for human activity recognition in smart
    homes.¬†Journal of Ambient Intelligence and Humanized Computing.
    https://doi.org/10.1007/s12652-022-03798-w

15. Zhang, L., Hu, H., & Zhang, D. (2015). A credit risk assessment
    model based on SVM for small and medium enterprises in supply chain
    finance.¬†*Financial Innovation*,¬†*1*(1).
    <https://doi.org/10.1186/s40854-015-0014-5>

16. Harimoorthy, K., Thangavelu, M. RETRACTED ARTICLE: Multi-disease
    prediction model using improved SVM-radial bias technique in
    healthcare monitoring system.¬†*J Ambient Intell Human
    Comput*¬†**12**, 3715‚Äì3723 (2021).
    https://doi.org/10.1007/s12652-019-01652-0

17. J. Liang, Z. Qin, L. Xue, X. Lin and X. Shen, "Verifiable and Secure
    SVM Classification for Cloud-Based Health Monitoring Services," in
    IEEE Internet of Things Journal, vol. 8, no. 23, pp. 17029-17042, 1
    Dec.1, 2021, doi: 10.1109/JIOT.2021.3075540.

18. G. N. Ahmad, H. Fatima, S. Ullah, A. Salah Saidi and Imdadullah,
    "Efficient Medical Diagnosis of Human Heart Diseases Using Machine
    Learning Techniques With and Without GridSearchCV," in IEEE Access,
    vol. 10, pp. 80151-80173, 2022, doi: 10.1109/ACCESS.2022.3165792.

19. Sahara, S., Annida Purnamawati, Sulaeman Hadi Sukmana, Mely
    Mailasari, Erma Delima Sikumbang, & Puji, E. (2023). PSO
    optimization for analysis of online marketplace products on the SVM
    method. *AIP Conference Proceedings*.
    https://doi.org/10.1063/5.0129404

20. ‚ÄúPrediction of Consumer Purchasing in a Grocery Store Using Machine
    Learning Techniques.‚Äù *Ieeexplore.ieee.org*,
    ieeexplore.ieee.org/document/7941935.

21. Barakat, Nahla, et al. ‚ÄúIntelligible Support Vector Machines for
    Diagnosis of Diabetes Mellitus.‚Äù *IEEE Transactions on Information
    Technology in Biomedicine*, vol. 14, no. 4, July 2010, pp.
    1114‚Äì1120, https://doi.org/10.1109/titb.2009.2039485.

22. ‚ÄúApplying Support Vector Machine to Electronic Health Records for
    Cancer Classification \| IEEE Conference Publication \| IEEE
    Xplore.‚Äù *Ieeexplore.ieee.org*,
    ieeexplore.ieee.org/abstract/document/8732906.

23. ‚ÄúAn Effective Intrusion Detection Approach Using SVM with Na√Øve
    Bayes Feature Embedding.‚Äù *Computers & Security*, vol. 103, 1 Apr.
    2021, p. 102158,
    www.sciencedirect.com/science/article/pii/S0167404820304314,
    https://doi.org/10.1016/j.cose.2020.102158.

24. Hosseini, Soodeh, and Behnam Mohammad Hasani Zade. ‚ÄúNew Hybrid
    Method for Attack Detection Using Combination of Evolutionary
    Algorithms, SVM, and ANN.‚Äù *Computer Networks*, vol. 173, May
    2020, p. 107168, https://doi.org/10.1016/j.comnet.2020.107168.
